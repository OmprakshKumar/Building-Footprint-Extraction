{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Data Files and creating .npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "import cv2\n",
    "\n",
    "data_path = \"E:/projects/semantic segmentation/data/\"\n",
    "\n",
    "image_rows = 370\n",
    "image_cols = 400\n",
    "channel = 3\n",
    "#image_rows = 512\n",
    "#image_cols = 512\n",
    "\n",
    "\n",
    "def create_train_data():\n",
    "    train_data_path = os.path.join(data_path, 'train')\n",
    "    images = os.listdir(train_data_path)\n",
    "    total = int(len(images) / 2)\n",
    "\n",
    "    imgs = np.ndarray((total, image_rows, image_cols, channel), dtype=np.uint8)\n",
    "    imgs_mask = np.ndarray((total, image_rows, image_cols, channel), dtype=np.uint8)\n",
    "\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    for image_name in images:\n",
    "        if 'mask' in image_name:\n",
    "            continue\n",
    "        image_mask_name = image_name.split('.')[0] + '_mask.png'\n",
    "        # img = np.array(Image.open(os.path.join(train_data_path, image_name)))\n",
    "        # img_mask = np.array(Image.open(os.path.join(train_data_path, image_mask_name)))\n",
    "        # img = imread(os.path.join(train_data_path, image_name), as_grey=True)\n",
    "        # img_mask = imread(os.path.join(train_data_path, image_mask_name), as_grey=True)\n",
    "        img = cv2.imread(os.path.join(train_data_path, image_name))\n",
    "        # print (img.shape)\n",
    "        img_mask = cv2.imread(os.path.join(train_data_path, image_mask_name))\n",
    "\n",
    "        img = np.array([img])\n",
    "        img_mask = np.array([img_mask])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_mask[i] = img_mask\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "\n",
    "    np.save(data_path + 'file/train.npy', imgs)\n",
    "    np.save(data_path + 'file/train_mask.npy', imgs_mask)\n",
    "    print('Saving to .npy files done.')\n",
    "\n",
    "def create_test_data():\n",
    "    train_data_path = os.path.join(data_path, 'test')\n",
    "    images = os.listdir(train_data_path)\n",
    "    total = int(len(images) / 2)\n",
    "\n",
    "    imgs = np.ndarray((total, image_rows, image_cols, channel), dtype=np.uint8)\n",
    "    imgs_mask = np.ndarray((total, image_rows, image_cols, channel), dtype=np.uint8)\n",
    "\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    for image_name in images:\n",
    "        if 'mask' in image_name:\n",
    "            continue\n",
    "        image_mask_name = image_name.split('.')[0] + '_mask.png'\n",
    "        # img = imread(os.path.join(train_data_path, image_name), as_grey=True)\n",
    "        # img_mask = imread(os.path.join(train_data_path, image_mask_name), as_grey=True)\n",
    "        img = cv2.imread(os.path.join(train_data_path, image_name))\n",
    "        img_mask = cv2.imread(os.path.join(train_data_path, image_mask_name))\n",
    "\n",
    "        img = np.array([img])\n",
    "        img_mask = np.array([img_mask])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_mask[i] = img_mask\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "\n",
    "    np.save(data_path + 'file/test.npy', imgs)\n",
    "    np.save(data_path + 'file/test_mask.npy', imgs_mask)\n",
    "    print('Saving to .npy files done.')\n",
    "    \n",
    "\n",
    "def create_valid_data():\n",
    "    train_data_path = os.path.join(data_path, 'validation')\n",
    "    images = os.listdir(train_data_path)\n",
    "    total = int(len(images) / 2)\n",
    "\n",
    "    imgs = np.ndarray((total, image_rows, image_cols, channel), dtype=np.uint8)\n",
    "    imgs_mask = np.ndarray((total, image_rows, image_cols, channel), dtype=np.uint8)\n",
    "\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    for image_name in images:\n",
    "        if 'mask' in image_name:\n",
    "            continue\n",
    "        image_mask_name = image_name.split('.')[0] + '_mask.png'\n",
    "        # img = imread(os.path.join(train_data_path, image_name), as_grey=True)\n",
    "        # img_mask = imread(os.path.join(train_data_path, image_mask_name), as_grey=True)\n",
    "        img = cv2.imread(os.path.join(train_data_path, image_name))\n",
    "        img_mask = cv2.imread(os.path.join(train_data_path, image_mask_name))\n",
    "\n",
    "        img = np.array([img])\n",
    "        img_mask = np.array([img_mask])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_mask[i] = img_mask\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "\n",
    "    np.save(data_path + 'file/validation.npy', imgs)\n",
    "    np.save(data_path + 'file/validation_mask.npy', imgs_mask)\n",
    "    print('Saving to .npy files done.')\n",
    "\n",
    "create_train_data()\n",
    "create_test_data()\n",
    "create_valid_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import numpy as np\n",
    "import os\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "from keras.layers.normalization import BatchNormalization as bn\n",
    "from skimage.transform import resize\n",
    "from keras.models import Model\n",
    "from keras import regularizers \n",
    "from keras.layers import Input,merge, concatenate, Conv2D,ZeroPadding2D,Convolution2D, Conv2DTranspose,MaxPooling2D, UpSampling2D, add,multiply,Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.core import Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping,TensorBoard\n",
    "from keras import backend as K,models\n",
    "from skimage.io import imsave\n",
    "import nibabel as nib\n",
    "from medpy import metric\n",
    "from keras import losses\n",
    "import cv2\n",
    "import scipy as sp\n",
    "import scipy.misc, scipy.ndimage.interpolation\n",
    "def merge(inputs, mode, concat_axis=-1):\n",
    "    return concatenate(inputs, concat_axis)\n",
    "\n",
    "#K.set_image_data_format('channels_last')  # TF dimension ordering in this code\n",
    "smooth = 1.\n",
    "\n",
    "img_rows = 128\n",
    "img_cols = 128\n",
    "channel = 3\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_path = 'E:/projects/semantic segmentation/data/file/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_train_data():\n",
    "    imgs_train = np.load(data_path + 'train.npy')\n",
    "    imgs_mask_train = np.load(data_path + 'train_mask.npy')\n",
    "    return imgs_train, imgs_mask_train\n",
    "\n",
    "\n",
    "def load_validation_data():\n",
    "    imgs_valid = np.load(data_path + 'validation.npy')\n",
    "    imgs_mask_valid = np.load(data_path + 'validation_mask.npy')\n",
    "    return imgs_valid, imgs_mask_valid\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1.0-dice_coef(y_true, y_pred)\n",
    "\n",
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)  \n",
    "\n",
    "\n",
    "  \n",
    "def sensitivity(y_true,y_pred):\n",
    "    true_positives=tf.reduce_sum(tf.round(K.clip(y_true*y_pred, 0, 1)))\n",
    "    possible_positives=tf.reduce_sum(tf.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives+K.epsilon())\n",
    "def specificity(y_true,y_pred):\n",
    "    true_negatives=tf.reduce_sum(K.round(K.clip((1-y_true)*(1-y_pred), 0, 1)))\n",
    "    possible_negatives=tf.reduce_sum(K.round(K.clip((1-y_true), 0, 1)))\n",
    "    return true_negatives / (possible_negatives+K.epsilon())\n",
    "\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    " \n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))     \n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def encoder(x, filters=32, n_block=5, kernel_size=(3, 3), activation='relu'):\n",
    "    skip = []\n",
    "    for i in range(n_block):\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        skip.append(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
    "    return x, skip\n",
    "\n",
    "\n",
    "def bottleneck(x, filters_bottleneck, mode='cascade', depth=6,\n",
    "               kernel_size=(3, 3), activation='relu'):\n",
    "    dilated_layers = []\n",
    "    if mode == 'cascade':  # used in the competition\n",
    "        for i in range(depth):\n",
    "            x = Conv2D(filters_bottleneck, kernel_size,\n",
    "                       activation=activation, padding='same', dilation_rate=2**i)(x)\n",
    "            dilated_layers.append(x)\n",
    "        return add(dilated_layers)\n",
    "    elif mode == 'parallel':  # Like \"Atrous Spatial Pyramid Pooling\"\n",
    "        for i in range(depth):\n",
    "            dilated_layers.append(\n",
    "                Conv2D(filters_bottleneck, kernel_size,\n",
    "                       activation=activation, padding='same', dilation_rate=2**i)(x)\n",
    "            )\n",
    "        return add(dilated_layers)\n",
    "\n",
    "\n",
    "def decoder(x, skip, filters, n_block=5, kernel_size=(3, 3), activation='relu'):\n",
    "    for i in reversed(range(n_block)):\n",
    "        x = UpSampling2D(size=(2, 2))(x)\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        x = concatenate([skip[i], x])\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "        x = Conv2D(filters * 2**i, kernel_size, activation=activation, padding='same')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_dilated_unet(\n",
    "        input_shape=(128, 128, 3),\n",
    "        mode='cascade',\n",
    "        filters=32,\n",
    "        n_block=5,\n",
    "        lr=0.001,\n",
    "        loss=dice_coef_loss,\n",
    "        n_class=1\n",
    "):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    enc, skip = encoder(inputs, filters, n_block)\n",
    "    bottle = bottleneck(enc, filters_bottleneck=filters * 2**n_block, mode=mode)\n",
    "    dec = decoder(bottle, skip, filters, n_block)\n",
    "    classify = Conv2D(n_class, (1, 1), activation='sigmoid')(dec)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=classify)\n",
    "    model.compile(optimizer=Adam(lr), loss=loss, metrics=['accuracy',dice_coef,sensitivity,specificity,f1score,precision,recall, mean_iou])\n",
    "\n",
    "    return model\n",
    "\n",
    "   \n",
    "\n",
    "def preprocess(imgs):\n",
    "    imgs_p = np.ndarray((imgs.shape[0], img_rows, img_cols, channel), dtype=np.uint8)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i] = resize(imgs[i], (img_cols, img_rows, channel), preserve_range=True)\n",
    "\n",
    "    #imgs_p = imgs_p[...]\n",
    "    return imgs_p\n",
    "\n",
    "\n",
    "def train():\n",
    "    print('-' * 30)\n",
    "    print('Loading and preprocessing train data...')\n",
    "    print('-' * 30)\n",
    "    imgs_train, imgs_mask_train = load_train_data()\n",
    "    imgs_valid, imgs_mask_valid = load_validation_data()\n",
    "    imgs_train = preprocess(imgs_train)\n",
    "    print(imgs_train.shape)\n",
    "    imgs_mask_train = preprocess(imgs_mask_train)\n",
    "    print(imgs_mask_train.shape)\n",
    "    imgs_valid = preprocess(imgs_valid)\n",
    "    print(imgs_valid.shape)\n",
    "    imgs_mask_valid = preprocess(imgs_mask_valid)\n",
    "    print(imgs_mask_valid.shape)\n",
    "\n",
    "    imgs_train = imgs_train.astype('float32')\n",
    "    imgs_valid = imgs_valid.astype('float32')\n",
    "\n",
    "    mean = np.mean(imgs_train)  # mean for data centering\n",
    "    std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "    val_mean = np.mean(imgs_valid)\n",
    "    val_std = np.std(imgs_valid)\n",
    "\n",
    "    imgs_train -= mean\n",
    "    imgs_train /= std\n",
    "\n",
    "    imgs_valid -= val_mean\n",
    "    imgs_valid /= val_std\n",
    "\n",
    "    imgs_mask_train = imgs_mask_train.astype('float32')\n",
    "    imgs_mask_train /= 255.  # scale masks to [0, 1]\n",
    "\n",
    "    imgs_mask_valid = imgs_mask_valid.astype('float32')\n",
    "    imgs_mask_valid /= 255.\n",
    "\n",
    "    print('-' * 30)\n",
    "    print('Creating and compiling model...')\n",
    "    print('-' * 30)\n",
    "    model = get_dilated_unet()\n",
    "    model_checkpoint = ModelCheckpoint('E:/projects/semantic segmentation/data/file/dilatedunet.hdf5', monitor='val_loss',\n",
    "                                       save_best_only=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    print('-' * 30)\n",
    "    print('Fitting model...')\n",
    "    print('-' * 30)\n",
    "    earlystopper=EarlyStopping(monitor='val_loss',patience=10,verbose=1)\n",
    "    his = model.fit(imgs_train, imgs_mask_train, batch_size=32, epochs=200, verbose=1, shuffle=True,\n",
    "                    validation_data=(imgs_valid, imgs_mask_valid), callbacks=[model_checkpoint])\n",
    "    score_1=model.evaluate(imgs_train,imgs_mask_train,batch_size=32,verbose=1)\n",
    "    print(' Train loss:',score_1[0])\n",
    "    print(' Train accuracy:',score_1[1])\n",
    "    print(' Train dice_coef:',score_1[2])\n",
    "    print(' Train sensitivity:',score_1[3])\n",
    "    print(' Train specificity:',score_1[4])\n",
    "    print(' Train f1score:',score_1[5])\n",
    "    print('Train precision:',score_1[6])\n",
    "    print(' Train recall:',score_1[7])\n",
    "    print(' Train mean_iou:',score_1[8])\n",
    "    res_loss_1 = np.array(score_1)\n",
    "    np.savetxt(data_path+ 'res_loss_1.txt', res_loss_1)\n",
    "    \n",
    "    score_2=model.evaluate(imgs_valid,imgs_mask_valid,batch_size=32,verbose=1)\n",
    "    print(' valid loss:',score_2[0])\n",
    "    print(' valid  accuracy:',score_2[1])\n",
    "    print(' valid  dice_coef:',score_2[2])\n",
    "    print(' valid  sensitivity:',score_2[3])\n",
    "    print(' valid  specificity:',score_2[4])\n",
    "    print(' valid f1score:',score_2[5])\n",
    "    print('valid  precision:',score_2[6])\n",
    "    print(' valid  recall:',score_2[7])\n",
    "    print(' valid  mean_iou:',score_2[8])\n",
    "    \n",
    "    res_loss_2 = np.array(score_2)\n",
    "    np.savetxt(data_path + 'res_loss_2.txt', res_loss_2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.plot()\n",
    "    plt.plot(his.history['loss'], label='train loss')\n",
    "    plt.plot(his.history['val_loss'], c='g', label='val loss')\n",
    "    plt.title('train and val loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.plot()\n",
    "    plt.plot(his.history['acc'], label='train accuracy')\n",
    "    plt.plot(his.history['val_acc'], c='g', label='val accuracy')\n",
    "    plt.title('train  and val acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.plot()\n",
    "    plt.plot(his.history['dice_coef'], label='train dice_coef')\n",
    "    plt.plot(his.history['val_dice_coef'], c='g', label='val dice_coef')\n",
    "    plt.title('train  and val dice_coef')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "    plt.plot()\n",
    "    plt.plot(his.history['sensitivity'], label='train sensitivity')\n",
    "    plt.plot(his.history['val_sensitivity'], c='g', label='val sensitivity')\n",
    "    plt.title('train  and val sensitivity')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.plot()\n",
    "    plt.plot(his.history['specificity'], label='train specificity')\n",
    "    plt.plot(his.history['val_specificity'], c='g', label='val specificity')\n",
    "    plt.title('train  and val specificity')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.plot()\n",
    "    plt.plot(his.history['f1score'], label='train f1score')\n",
    "    plt.plot(his.history['val_f1score'], c='g', label='val f1score')\n",
    "    plt.title('train  and val f1score')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "   \n",
    "    plt.plot()\n",
    "    plt.plot(his.history['precision'], label='train precision')\n",
    "    plt.plot(his.history['val_precision'], c='g', label='val_precision')\n",
    "    plt.title('train  and val precision')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()\n",
    "   \n",
    "    plt.plot()\n",
    "    plt.plot(his.history['mean_iou'], label='Train mean_iou')\n",
    "    plt.plot(his.history['val_mean_iou'], c='g', label='val_mean_iou')\n",
    "    plt.title('train and val mean_iou')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    " \n",
    "     \n",
    "    plt.plot()\n",
    "    plt.plot(his.history['recall'], label='train recall')\n",
    "    plt.plot(his.history['val_recall'], c='g', label='val_recall')\n",
    "    plt.title('train  and val recall')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = get_dilated_unet()\n",
    "    print(model.summary())\n",
    "    train()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
